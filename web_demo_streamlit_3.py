import streamlit as st
import torch
from transformers import AutoModel, AutoTokenizer
# from peft import PeftModel # LoRA removed
from langchain_community.embeddings import HuggingFaceEmbeddings
try:
    from langchain_community.vectorstores import Neo4jVector
except ImportError:
    from langchain.vectorstores import Neo4jVector

# ================= é…ç½®è·¯å¾„ =================
# è¯·ç¡®ä¿è¿™äº›è·¯å¾„ä¸ä½  AutoDL ä¸Šçš„å®é™…è·¯å¾„ä¸€è‡´
# å¦‚æœåœ¨æœ¬åœ°è¿è¡Œï¼Œè¯·ä¿®æ”¹ä¸ºæœ¬åœ°è·¯å¾„
MODEL_PATH = "/root/autodl-tmp/models/chatglm3-6b"
EMBEDDING_PATH = "/root/autodl-tmp/models/bge-large-zh-v1.5"

# è®¾ç½®é¡µé¢
st.set_page_config(page_title="å·¥ä¸šæ–‡æ¡£çŸ¥è¯†é—®ç­” Agent", page_icon="ğŸ¤–", layout="wide")
st.title("ğŸ¤– å·¥ä¸šæ–‡æ¡£çŸ¥è¯†é—®ç­” Agent (Base Model + RAG)")
st.markdown("### ğŸš€ åŸºäº ChatGLM3-6B (æ—  LoRA) ä¸ Neo4j çŸ¥è¯†å›¾è°±")

# ================= 1. åŠ è½½æ¨¡å‹ =================
@st.cache_resource
def load_models():
    print("â³ [System] æ­£åœ¨åŠ è½½ Embedding æ¨¡å‹...")
    try:
        embedding = HuggingFaceEmbeddings(
            model_name=EMBEDDING_PATH,
            model_kwargs={'device': 'cuda' if torch.cuda.is_available() else 'cpu'}
        )
    except Exception as e:
        st.error(f"Embedding æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return None, None, None
    
    print("â³ [System] æ­£åœ¨è¿æ¥ Neo4j...")
    try:
        # æ­¤æ—¶è¿æ¥ Neo4jï¼Œæ³¨æ„ index_name å¿…é¡»ä¸ data_import.py ä¸­ä¸€è‡´
        vector_store = Neo4jVector.from_existing_graph(
            embedding=embedding,
            url="bolt://localhost:7687",
            username="neo4j",
            password="12345678",
            index_name="vector",
            node_label="Chunk",
            text_node_properties=["text"],
            embedding_node_property="embedding",
        )
    except Exception as e:
        st.error(f"Neo4j è¿æ¥å¤±è´¥: {e}")
        return None, None, None

    print("â³ [System] æ­£åœ¨åŠ è½½ ChatGLM3 æ¨¡å‹...")
    try:
        tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)
        # æ»¡è¡€ FP16 åŠ è½½ï¼Œé€‚åˆ 3090/4090 æ˜¾å¡
        model = AutoModel.from_pretrained(MODEL_PATH, trust_remote_code=True, device_map="auto", torch_dtype="auto").eval()
    except Exception as e:
        st.error(f"ChatGLM3 æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return None, None, None

    # LoRA åŠ è½½é€»è¾‘å·²ç§»é™¤
    
    return tokenizer, model, vector_store

tokenizer, model, vector_store = load_models()

# ================= 2. çŠ¶æ€ç®¡ç† =================
if "history" not in st.session_state:
    st.session_state.history = []

# ================= 3. ä¾§è¾¹æ ä¸åŠŸèƒ½ =================
with st.sidebar:
    st.header("âš™ï¸ æ§åˆ¶é¢æ¿")
    # å¼ºåˆ¶æ¸…ç©ºæŒ‰é’®ï¼šè§£å†³ä¹±ç çš„å…³é”®
    if st.button("ğŸ—‘ï¸ æ¸…ç©ºå¯¹è¯å†å²"):
        st.session_state.history = []
        st.rerun()
    st.info("ğŸ’¡ å¦‚æœå›å¤å‡ºç°ä¹±ç ï¼Œè¯·ç‚¹å‡»ä¸Šæ–¹â€œæ¸…ç©ºå¯¹è¯å†å²â€æŒ‰é’®ã€‚")

# æ˜¾ç¤ºå†å²æ¶ˆæ¯
for query, response in st.session_state.history:
    with st.chat_message("user"):
        st.markdown(query)
    with st.chat_message("assistant"):
        st.markdown(response)

# ================= 4. æ ¸å¿ƒé—®ç­”é€»è¾‘ =================
if prompt_text := st.chat_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜..."):
    with st.chat_message("user"):
        st.markdown(prompt_text)

    # RAG æ£€ç´¢
    context_str = ""
    print(f"ğŸ” ç”¨æˆ·æé—®: {prompt_text}")
    try:
        with st.status("ğŸ” æ­£åœ¨æ£€ç´¢çŸ¥è¯†åº“ (Neo4j)...", expanded=False) as status:
            if vector_store:
                docs = vector_store.similarity_search(prompt_text, k=3)
                if docs:
                    context_str_list = []
                    status.write("âœ… æ£€ç´¢åˆ°ç›¸å…³çŸ¥è¯†ç‰‡æ®µï¼š")
                    for i, d in enumerate(docs):
                        # è·å–å…ƒæ•°æ®
                        source = d.metadata.get('source', 'æœªçŸ¥æ–‡ä»¶')
                        chapter = d.metadata.get('chapter', 'æœªçŸ¥ç« èŠ‚')
                        page = d.metadata.get('page', '?')
                        
                        source_info = f"ã€æ¥æºã€‘{source} - {chapter} (P{page})"
                        status.markdown(f"**ç‰‡æ®µ {i+1}:** {source_info}")
                        status.code(d.page_content[:200] + "...", language="text")
                        
                        context_str_list.append(f"Content: {d.page_content}\nSource: {source_info}")
                    
                    context_str = "\n\n".join(context_str_list)
                else:
                    status.write("âš ï¸ æœªæ£€ç´¢åˆ°ç›´æ¥ç›¸å…³å†…å®¹ï¼Œå°†å°è¯•ä½¿ç”¨æ¨¡å‹é€šç”¨çŸ¥è¯†å›ç­”ã€‚")
                status.update(label="æ£€ç´¢å®Œæˆ", state="complete", expanded=False)
            else:
                status.write("âš ï¸ å‘é‡åº“æœªè¿æ¥ï¼Œä»…ä½¿ç”¨æ¨¡å‹å›ç­”ã€‚")
                status.update(label="æ£€ç´¢è·³è¿‡", state="error", expanded=False)
                
    except Exception as e:
        st.error(f"æ£€ç´¢å‡ºé”™: {e}")

    # æ„é€  Prompt
    input_prompt = f"åŸºäºä»¥ä¸‹å·²çŸ¥ä¿¡æ¯ï¼Œç®€æ´ä¸“ä¸šåœ°å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚å¦‚æœä¸ç¡®å®šï¼Œè¯·è¯´æ˜ã€‚\n\nã€å·²çŸ¥ä¿¡æ¯ã€‘\n{context_str}\n\nã€ç”¨æˆ·é—®é¢˜ã€‘\n{prompt_text}"

    with st.chat_message("assistant"):
        placeholder = st.empty()
        full_response = ""
        
        if model:
            try:
                # å…³é”®å‚æ•°è®¾ç½®ï¼šé˜²æ­¢ä¹±ç å’ŒæŠ¥é”™
                for response, history, past_key_values in model.stream_chat(
                    tokenizer, 
                    input_prompt, 
                    history=st.session_state.history, 
                    do_sample=False,        # å…³é—­éšæœºé‡‡æ ·ï¼Œè§£å†³ NaN æŠ¥é”™
                    repetition_penalty=1.2, # æ ¸å¿ƒï¼šæƒ©ç½šé‡å¤ï¼Œè§£å†³æ–¹å—å­—ä¹±ç 
                    max_length=4096,
                    past_key_values=None,
                    return_past_key_values=True
                ):
                    placeholder.markdown(response)
                    full_response = response
                
                st.session_state.history = history
                
            except Exception as e:
                st.error(f"ç”Ÿæˆå‡ºé”™: {e}")
                print(f"âŒ ç”Ÿæˆå‡ºé”™: {e}")
        else:
            st.error("æ¨¡å‹æœªåŠ è½½ï¼Œæ— æ³•ç”Ÿæˆå›ç­”ã€‚")
