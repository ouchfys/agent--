import os
import sys
import pandas as pd
from datasets import Dataset
from ragas import evaluate
from ragas.metrics import (
    context_precision,
    context_recall,
    faithfulness,
    answer_relevancy,
)
from langchain_openai import ChatOpenAI
from langchain_community.vectorstores import Neo4jVector
from langchain_community.embeddings import HuggingFaceEmbeddings
from transformers import AutoTokenizer, AutoModel
import torch

# æ·»åŠ  system ç›®å½•ä»¥å¯¼å…¥ file_extraction
sys.path.append(os.path.join(os.path.dirname(__file__), 'system'))
try:
    from file_extraction import extract_text_from_pdf
except ImportError:
    # å°è¯•ç›´æ¥å¯¼å…¥
    from file_extraction import extract_text_from_pdf

# ================= 1. é…ç½®åŒºåŸŸ (è¯·å¡«å…¥ä½ çš„ DeepSeek Key) =================
# âš ï¸âš ï¸âš ï¸åœ¨æ­¤å¤„å¡«å…¥ä½ çš„ DeepSeek API Key âš ï¸âš ï¸âš ï¸
DEEPSEEK_API_KEY = "sk-cc62600607034908acd7e755ffef5e66" 

# æœ¬åœ°æ¨¡å‹è·¯å¾„
MODEL_PATH = "/root/autodl-tmp/models/chatglm3-6b"
EMBEDDING_PATH = "/root/autodl-tmp/models/bge-large-zh-v1.5"

# Neo4j é…ç½®
NEO4J_URL = "bolt://localhost:7687"
NEO4J_USER = "neo4j"
NEO4J_PASSWORD = "12345678"

# ================= 2. åˆå§‹åŒ–æ¨¡å‹ =================

# --- A. è€ƒå®˜ (DeepSeek) ---
print("ğŸ‘¨â€âš–ï¸ æ­£åœ¨åˆå§‹åŒ– DeepSeek è€ƒå®˜...")
judge_llm = ChatOpenAI(
    model="deepseek-chat",            # DeepSeek V3
    openai_api_key=DEEPSEEK_API_KEY,
    openai_api_base="https://api.deepseek.com", # DeepSeek å®˜æ–¹æ¥å£åœ°å€
    temperature=0 # è¯„ä¼°æ—¶ä¿æŒå†·é™ï¼Œä¸è¦éšæœº
)

# --- B. è€ƒç”Ÿ (æœ¬åœ° ChatGLM3) ---
class ChatGLM3Wrapper:
    """ç®€å•çš„ ChatGLM3 åŒ…è£…å™¨ï¼Œç”¨äºç”Ÿæˆå›ç­”"""
    def __init__(self):
        print("â³ æ­£åœ¨åŠ è½½æœ¬åœ° ChatGLM3 (è€ƒç”Ÿ)...")
        self.tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)
        self.model = AutoModel.from_pretrained(MODEL_PATH, trust_remote_code=True, device_map="auto", torch_dtype="auto").eval()

    def invoke(self, prompt: str) -> str:
        response, _ = self.model.chat(self.tokenizer, prompt, history=[], do_sample=False)
        return response

local_llm = ChatGLM3Wrapper()

# --- C. å‘é‡æ£€ç´¢ (Neo4j) ---
print("â³ æ­£åœ¨è¿æ¥ Neo4j çŸ¥è¯†åº“...")
embedding_model = HuggingFaceEmbeddings(
    model_name=EMBEDDING_PATH,
    model_kwargs={'device': 'cuda' if torch.cuda.is_available() else 'cpu'},
    encode_kwargs={'normalize_embeddings': True}
)

vector_store = Neo4jVector.from_existing_graph(
    embedding=embedding_model,
    url=NEO4J_URL,
    username=NEO4J_USER,
    password=NEO4J_PASSWORD,
    index_name="vector",
    node_label="Chunk",
    text_node_properties=["text"],
    embedding_node_property="embedding",
)

# ================= 3. ç”Ÿæˆæµ‹è¯•æ•°æ® =================
def generate_test_data_with_deepseek(pdf_path, num=3):
    """
    è®© DeepSeek é˜…è¯»è®ºæ–‡ç‰‡æ®µï¼Œå¹¶ç”Ÿæˆé«˜è´¨é‡çš„é—®é¢˜å’Œæ ‡å‡†ç­”æ¡ˆ (Ground Truth)
    """
    print(f"ğŸ“„ æ­£åœ¨è¯»å–è®ºæ–‡: {os.path.basename(pdf_path)}")
    chunks = extract_text_from_pdf(pdf_path)
    
    if not chunks:
        print("âŒ æœªæå–åˆ°æ–‡æœ¬ï¼Œè¯·æ£€æŸ¥ file_extraction.py")
        return []

    import random
    # éšæœºé€‰å‡ ä¸ªç‰‡æ®µ
    selected_chunks = random.sample(chunks, min(len(chunks), num))
    
    test_set = []
    print("ğŸ§  DeepSeek æ­£åœ¨å‡ºé¢˜...")
    
    for chunk in selected_chunks:
        context = chunk.page_content
        
        # è®© DeepSeek å‡ºé¢˜
        prompt = f"""
        ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è€ƒå®˜ã€‚è¯·æ ¹æ®ä»¥ä¸‹æŠ€æœ¯æ–‡æ¡£ç‰‡æ®µï¼Œç”Ÿæˆä¸€ä¸ªå…·ä½“çš„ã€æœ‰æ·±åº¦çš„é—®é¢˜ï¼Œå¹¶æ ¹æ®æ–‡æ¡£å†…å®¹ç»™å‡ºæ ‡å‡†ç­”æ¡ˆã€‚
        
        ã€æ–‡æ¡£ç‰‡æ®µã€‘ï¼š
        {context}
        
        è¯·ä¸¥æ ¼æŒ‰ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼ˆä¸è¦è¾“å‡ºå…¶ä»–åºŸè¯ï¼‰ï¼š
        é—®é¢˜ï¼šxxx
        ç­”æ¡ˆï¼šxxx
        """
        
        response = judge_llm.invoke(prompt).content
        
        # ç®€å•çš„è§£æé€»è¾‘
        try:
            q_part = response.split("é—®é¢˜ï¼š")[1].split("ç­”æ¡ˆï¼š")[0].strip()
            a_part = response.split("ç­”æ¡ˆï¼š")[1].strip()
            
            test_set.append({
                "question": q_part,
                "ground_truth": a_part
            })
            print(f"  âœ… ç”Ÿæˆé¢˜ç›®: {q_part[:20]}...")
        except:
            print("  âš ï¸ è§£æé¢˜ç›®å¤±è´¥ï¼Œè·³è¿‡ä¸€æ¡")
            
    return test_set

# ================= 4. è¿è¡Œè¯„ä¼° =================
def run_evaluation(test_data):
    print("\nğŸš€ å¼€å§‹ RAG è€ƒè¯• (Local ChatGLM3 ä½œç­”)...")
    
    questions = []
    answers = []
    contexts = []
    ground_truths = []
    
    for item in test_data:
        q = item["question"]
        gt = item["ground_truth"]
        
        # 1. æ£€ç´¢
        docs = vector_store.similarity_search(q, k=3)
        retrieved_text = [d.page_content for d in docs]
        
        # 2. æœ¬åœ°æ¨¡å‹ç”Ÿæˆå›ç­”
        context_str = "\n".join(retrieved_text)
        prompt = f"åŸºäºå·²çŸ¥ä¿¡æ¯ï¼š\n{context_str}\n\né—®é¢˜ï¼š{q}"
        ans = local_llm.invoke(prompt)
        
        questions.append(q)
        answers.append(ans)
        contexts.append(retrieved_text)
        ground_truths.append(gt) # Ragas éœ€è¦ list of strings for GT? No, usually just string in list
        
    # æ„å»ºæ•°æ®é›†
    data_dict = {
        "question": questions,
        "answer": answers,
        "contexts": contexts,
        "ground_truth": ground_truths
    }
    dataset = Dataset.from_dict(data_dict)
    
    print("\nğŸ‘¨â€âš–ï¸ DeepSeek æ­£åœ¨é˜…å· (è¿è¡Œ Ragas æŒ‡æ ‡)...")
    # å…³é”®ç‚¹ï¼šæŠŠ judge_llm ä¼ ç»™ Ragas
    results = evaluate(
        dataset=dataset,
        metrics=[
            context_recall,
            faithfulness,
            answer_relevancy,
            context_precision
        ],
        llm=judge_llm,       # <--- DeepSeek åšè£åˆ¤
        embeddings=embedding_model # ä¾ç„¶ç”¨æœ¬åœ° BGE åšå‘é‡è®¡ç®—
    )
    
    return results

if __name__ == "__main__":
    # é»˜è®¤è·¯å¾„
    default_pdf = "/root/autodl-tmp/agenté¡¹ç›®/data/è®ºæ–‡ï¼ˆæ— è‹±æ–‡æ–‡çŒ®ï¼‰.pdf"
    
    if len(sys.argv) > 1:
        pdf_path = sys.argv[1]
    else:
        pdf_path = default_pdf
    
    # 1. å‡ºé¢˜
    test_data = generate_test_data_with_deepseek(pdf_path, num=3) # æ¼”ç¤ºç”¨3æ¡ï¼Œæ­£å¼å¯ä»¥ç”¨10æ¡
    
    if test_data:
        # 2. è€ƒè¯• & é˜…å·
        results = run_evaluation(test_data)
        
        print("\nğŸ† è¯„ä¼°æŠ¥å‘Š:")
        print(results)
        
        # ä¿å­˜
        df = results.to_pandas()
        df.to_csv("deepseek_eval_report.csv", index=False)
        print("âœ… ç»“æœå·²ä¿å­˜è‡³ deepseek_eval_report.csv")
    else:
        print("âŒ æœªç”Ÿæˆæµ‹è¯•æ•°æ®")